import sys
import tempfile
import os
import zipfile
import tarfile
import pandas as pd
import numpy as np

from accelerate import Accelerator

import anndata as ad

os.chdir("UCE")
sys.path.append(".")
from data_proc.data_utils import process_raw_anndata

## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
    "input": "resources_test/task_batch_integration/cxg_immune_cell_atlas/dataset.h5ad",
    "output": "output.h5ad",
}
meta = {
  'name': 'uce'
}
## VIASH END


print(">>> Reading input...", flush=True)
sys.path.append(meta["resources_dir"])
from read_anndata_partial import read_anndata

adata = read_anndata(par["input"], X="layers/counts", obs="obs", var="var", uns="uns")

print("\n>>> Creating working directory...", flush=True)
work_dir = tempfile.TemporaryDirectory()
print(f"Working directory: '{work_dir.name}'", flush=True)

print("\n>>> Getting model files...", flush=True)
if os.path.isdir(par["model"]):
    model_temp = None
    model_dir = par["model"]
else:
    model_temp = tempfile.TemporaryDirectory()
    model_dir = model_temp.name

    if zipfile.is_zipfile(par["model"]):
        print("Extracting UCE model from .zip...", flush=True)
        with zipfile.ZipFile(par["model"], "r") as zip_file:
            zip_file.extractall(model_dir)
    elif tarfile.is_tarfile(par["model"]) and par["model"].endswith(".tar.gz"):
        print("Extracting model from .tar.gz...", flush=True)
        with tarfile.open(par["model"], "r:gz") as tar_file:
            tar_file.extractall(model_dir)
            model_dir = os.path.join(model_dir, os.listdir(model_dir)[0])
    else:
        raise ValueError(
            f"The 'model' argument should be a directory a .zip file or a .tar.gz file"
        )

print("Extracting protein embeddings...", flush=True)
with tarfile.open(os.path.join(model_dir, "protein_embeddings.tar.gz"), "r:gz") as tar_file:
    tar_file.extractall("./model_files")
print(f"Model directory: '{model_dir}'", flush=True)

model_args = {
    "dir" : work_dir.name,
    "skip" : True,
    "filter" : False # Turn this off to get embedding for all cells
}

accelerator = Accelerator(model_args["dir"])

print("\n>>> Preprocessing data...", flush=True)
# Set var names to gene symbols
adata.var_names = adata.var["feature_name"]
adata.write_h5ad(os.path.join(model_args["dir"], "input.h5ad"))

row = pd.Series()
row.path = "input.h5ad"
row.covar_col = np.nan
if adata.uns["dataset_organism"] == "homo_sapiens":
    row.species = "human"
elif adata.uns["dataset_organism"] == "mus_musculus":
    row.species = "mouse"
else:
    raise ValueError(f"Species '{adata.uns['dataset_organism']} not yet implemented")

processed_adata, num_cells, num_genes = process_raw_anndata(
    row = row,
    h5_folder_path = model_args["dir"],
    npz_folder_path = model_args["dir"],
    scp = "",
    skip = model_args["skip"],
    additional_filter = model_args["filter"],
    root = model_args["dir"]
)

# processor.generate_idxs()
# processor.run_evaluation()

print('Generate predictions', flush=True)
# ... generate predictions ...

print("Write output AnnData to file", flush=True)
output = ad.AnnData(

)
output.write_h5ad(par['output'], compression='gzip')

print("\n>>> Cleaning up temporary directories...", flush=True)
work_dir.cleanup()
if model_temp is not None:
    model_temp.cleanup()

print("\n>>> Done!", flush=True)
