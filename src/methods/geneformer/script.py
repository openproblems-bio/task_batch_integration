import anndata as ad
from geneformer import TranscriptomeTokenizer, EmbExtractor
import os
import sys
from tempfile import TemporaryDirectory
from huggingface_hub import hf_hub_download
import numpy as np

## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
    "input": "resources_test/task_batch_integration/cxg_immune_cell_atlas/dataset.h5ad",
    "output": "output.h5ad",
}
meta = {"name": "geneformer"}
## VIASH END

n_processors = os.cpu_count()

print("Reading input", flush=True)
sys.path.append(meta["resources_dir"])
from read_anndata_partial import read_anndata

adata = read_anndata(par["input"], X="layers/counts", obs="obs", var="var", uns="uns")

if adata.uns["dataset_organism"] != "homo_sapiens":
    raise ValueError(
        f"Geneformer can only be used with human data "
        f"(dataset_organism == \"{adata.uns['dataset_organism']}\")"
    )

is_ensembl = all(var_name.startswith("ENSG") for var_name in adata.var_names)
if not is_ensembl:
    raise ValueError(f"Geneformer requires adata.var_names to contain ENSEMBL gene ids")

print("Creating working directory", flush=True)
work_dir = TemporaryDirectory()
input_dir = os.path.join(work_dir.name, "input")
os.makedirs(input_dir)
tokenized_dir = os.path.join(work_dir.name, "tokenized")
os.makedirs(tokenized_dir)
embedding_dir = os.path.join(work_dir.name, "embedding")
os.makedirs(embedding_dir)
print(f"Working directory: {work_dir.name}", flush=True)

print("Preparing data", flush=True)
adata.var["ensembl_id"] = adata.var_names
adata.obs["n_counts"] = np.ravel(adata.X.sum(axis=1))
adata.write_h5ad(os.path.join(input_dir, "input.h5ad"))
print(adata)

print("Getting dictionary files", flush=True)
# Mapping files for the 30M model
dictionary_files = {
    "ensembl_mapping": hf_hub_download(
        repo_id="ctheodoris/Geneformer",
        subfolder="geneformer/gene_dictionaries_30m",
        filename="ensembl_mapping_dict_gc30M.pkl",
    ),
    "gene_median": hf_hub_download(
        repo_id="ctheodoris/Geneformer",
        subfolder="geneformer/gene_dictionaries_30m",
        filename="gene_median_dictionary_gc30M.pkl",
    ),
    "gene_name_id": hf_hub_download(
        repo_id="ctheodoris/Geneformer",
        subfolder="geneformer/gene_dictionaries_30m",
        filename="gene_name_id_dict_gc30M.pkl",
    ),
    "token": hf_hub_download(
        repo_id="ctheodoris/Geneformer",
        subfolder="geneformer/gene_dictionaries_30m",
        filename="token_dictionary_gc30M.pkl",
    ),
}

print("Tokenizing data", flush=True)
# Set parameters for the 30M model
model_input_size = 2048
special_token = False
tokenizer = TranscriptomeTokenizer(
    nproc=n_processors,
    model_input_size=model_input_size,
    special_token=special_token,
    gene_median_file=dictionary_files["gene_median"],
    token_dictionary_file=dictionary_files["token"],
    gene_mapping_file=dictionary_files["ensembl_mapping"],
)

tokenizer.tokenize_data(input_dir, tokenized_dir, "tokenized", file_format="h5ad")

print("Getting model files", flush=True)
model_files = {
    "model": hf_hub_download(
        repo_id="ctheodoris/Geneformer",
        subfolder="gf-6L-30M-i2048",
        filename="model.safetensors",
    ),
    "config": hf_hub_download(
        repo_id="ctheodoris/Geneformer",
        subfolder="gf-6L-30M-i2048",
        filename="config.json",
    ),
}
model_dir = os.path.dirname(model_files["model"])

print("Extracting embeddings", flush=True)
embedder = EmbExtractor(
    emb_mode="cell", max_ncells=None, token_dictionary_file=dictionary_files["token"]
)
embedder.extract_embs(
    model_dir,
    os.path.join(tokenized_dir, "tokenized.dataset"),
    embedding_dir,
    "embedding",
)

# TODO: Get embedding from output directory, store and save output

print("Write output AnnData to file", flush=True)
output = ad.AnnData()
output.write_h5ad(par["output"], compression="gzip")
