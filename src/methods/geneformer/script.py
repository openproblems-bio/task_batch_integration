import anndata as ad
from geneformer import TranscriptomeTokenizer
from tempfile import TemporaryDirectory
import os
import requests

## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
  'input': 'resources_test/task_batch_integration/cxg_immune_cell_atlas/dataset.h5ad',
  'output': 'output.h5ad'
}
meta = {
  'name': 'geneformer'
}
## VIASH END

print('Reading input files', flush=True)
input = ad.read_h5ad(par['input'])

n_processors = os.cpu_count()

# Mapping files for the 30M model
base_dictionary_url = "https://huggingface.co/ctheodoris/Geneformer/blob/main/geneformer/gene_dictionaries_30m"
dictionary_files = {
    "ensembl_mapping" : "ensembl_mapping_dict_gc30M.pkl",
    "gene_median" : "gene_median_dictionary_gc30M.pkl",
    "gene_name_id" : "gene_name_id_dict_gc30M.pkl",
    "token" : "token_dictionary_gc30M.pkl"
}
dictionary_dir = TemporaryDirectory()
for file in dictionary_files.values():
    url = os.path.join(base_dictionary_url, file)
    response = requests.get(url)
    with open(os.path.join(dictionary_dir.name, file), 'wb') as f:
        f.write(response.content)

# Set parameters for the 30M model
model_input_size = 2048
special_token = False
tokenizer = TranscriptomeTokenizer(nproc = n_processors, model_input_size = model_input_size, special_token = special_token)
# tokenizer.tokenize_data(data_directory, output_directory, output_prefix, file_format = "anndata")

print('Preprocess data', flush=True)
# ... preprocessing ...

print('Train model', flush=True)
# ... train model ...

print('Generate predictions', flush=True)
# ... generate predictions ...

print("Write output AnnData to file", flush=True)
output = ad.AnnData(

)
output.write_h5ad(par['output'], compression='gzip')
