import anndata as ad
import scanpy as sc
import drvi
from drvi.model import DRVI
from drvi.utils.misc import hvg_batch
import pandas as pd
import numpy as np
import warnings
import sys
import scipy.sparse

## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
  'input': 'resources_test/task_batch_integration/cxg_immune_cell_atlas/dataset.h5ad',
  'output': 'output.h5ad',
  'n_hvg': 2000,
  'n_epochs': 400
}
meta = {
  'name': 'drvi'
}
## VIASH END

sys.path.append(meta["resources_dir"])
from read_anndata_partial import read_anndata

print('Reading input files', flush=True)
adata = read_anndata(
    par['input'],
    X='layers/counts',
    obs='obs',
    var='var',
    uns='uns'
)
# Remove dataset with non-count values
counts = adata.X
import scipy.sparse

if scipy.sparse.issparse(counts):
    counts_dense = counts.toarray()
else:
    counts_dense = counts

if not np.allclose(counts_dense, np.round(counts_dense)):
    warnings.warn("Non-integer values detected. Rounding to nearest integer.")
    adata.X = np.round(counts_dense).astype(int)

adata.layers["counts"] = adata.X.copy()

if par["n_hvg"]: 
     print(f"Select top {par['n_hvg']} high variable genes", flush=True) 
     idx = adata.var["hvg_score"].to_numpy().argsort()[::-1][:par["n_hvg"]] 
     adata = adata[:, idx].copy() 

print('Train model with DRVI', flush=True)
# Setup data
DRVI.setup_anndata(
    adata,
    # DRVI accepts count data by default.
    # Do not forget to change gene_likelihood if you provide a non-count data.
    layer="counts",
    # Always provide a list. DRVI can accept multiple covariates.
    categorical_covariate_keys=["batch"],
    # DRVI accepts count data by default.
    # Set to false if you provide log-normalized data and use normal distribution (mse loss).
    is_count_data=False,
)

# construct the model
model = DRVI(
    adata,
    # Provide categorical covariates keys once again. Refer to advanced usages for more options.
    categorical_covariates=["batch"],
    n_latent=128,
    # For encoder and decoder dims, provide a list of integers.
    encoder_dims=[128, 128],
    decoder_dims=[128, 128],
)
model

# train the model
model.train(
    max_epochs=par["n_epochs"],
    early_stopping=False,
    early_stopping_patience=20,
    # mps
    # accelerator="mps", devices=1,
    # cpu
    # accelerator="cpu", devices=1,
    # gpu: no additional parameter
    #
    # No need to provide `plan_kwargs` if n_epochs >= 400.
    plan_kwargs={
        "n_epochs_kl_warmup": par["n_epochs"],
    },
)

embed = ad.AnnData(model.get_latent_representation(), obs=adata.obs)

print("Store outputs", flush=True)
output = ad.AnnData(
    obs=adata.obs.copy(),
    var=adata.var.copy(),
    obsm={
        "X_emb": model.get_latent_representation(),
    },
    uns={
        "dataset_id": adata.uns.get("dataset_id", "unknown"),
        "normalization_id": adata.uns.get("normalization_id", "unknown"),
        "method_id": meta["name"],
    },
)

print("Write output AnnData to file", flush=True)
output.write_h5ad(par['output'], compression='gzip')
