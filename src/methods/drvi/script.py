import anndata as ad
import scanpy as sc
import drvi
from drvi.model import DRVI
from drvi.utils.misc import hvg_batch
import pandas as pd

## VIASH START
# Note: this section is auto-generated by viash at runtime. To edit it, make changes
# in config.vsh.yaml and then run `viash config inject config.vsh.yaml`.
par = {
  'input': 'resources_test/task_batch_integration/cxg_immune_cell_atlas/dataset.h5ad',
  'output': 'output.h5ad'
}
meta = {
  'name': 'drvi'
}
## VIASH END

print('Reading input files', flush=True)
adata = ad.read_h5ad(par['input'])
# Remove dataset with non-count values
adata = adata[adata.obs["batch"] != "Villani"].copy()

print('Preprocess data', flush=True)
adata.X = adata.layers["counts"].copy()
sc.pp.normalize_total(adata)
sc.pp.log1p(adata)
adata

sc.pp.pca(adata)
sc.pp.neighbors(adata)
sc.tl.umap(adata)
adata

# Batch aware HVG selection (method is obtained from scIB metrics)
hvg_genes = hvg_batch(adata, batch_key="batch", target_genes=2000, adataOut=False)
adata = adata[:, hvg_genes].copy()
adata

print('Train model with drVI', flush=True)
# Setup data
DRVI.setup_anndata(
    adata,
    # DRVI accepts count data by default.
    # Do not forget to change gene_likelihood if you provide a non-count data.
    layer="counts",
    # Always provide a list. DRVI can accept multiple covariates.
    categorical_covariate_keys=["batch"],
    # DRVI accepts count data by default.
    # Set to false if you provide log-normalized data and use normal distribution (mse loss).
    is_count_data=False,
)

# construct the model
model = DRVI(
    adata,
    # Provide categorical covariates keys once again. Refer to advanced usages for more options.
    categorical_covariates=["batch"],
    n_latent=128,
    # For encoder and decoder dims, provide a list of integers.
    encoder_dims=[128, 128],
    decoder_dims=[128, 128],
)
model

n_epochs = 400

# train the model
model.train(
    max_epochs=n_epochs,
    early_stopping=False,
    early_stopping_patience=20,
    # mps
    # accelerator="mps", devices=1,
    # cpu
    # accelerator="cpu", devices=1,
    # gpu: no additional parameter
    #
    # No need to provide `plan_kwargs` if n_epochs >= 400.
    plan_kwargs={
        "n_epochs_kl_warmup": n_epochs,
    },
)

embed = ad.AnnData(model.get_latent_representation(), obs=adata.obs)
sc.pp.subsample(embed, fraction=1.0)  # Shuffling for better visualization of overlapping colors

sc.pp.neighbors(embed, n_neighbors=10, use_rep="X", n_pcs=embed.X.shape[1])
sc.tl.umap(embed, spread=1.0, min_dist=0.5, random_state=123)
sc.pp.pca(embed)

print("Store outputs", flush=True)
output = ad.AnnData(
    obs=adata.obs.copy(),
    var=adata.var.copy(),
    obsm={
        "X_emb": model.get_latent_representation(),
    },
    uns={
        "dataset_id": adata.uns.get("dataset_id", "unknown"),
        "normalization_id": adata.uns.get("normalization_id", "unknown"),
        "method_id": meta["name"],
    },
)

print("Write output AnnData to file", flush=True)
output.write_h5ad(par['output'], compression='gzip')
